{"cells":[{"cell_type":"markdown","source":["#README\n","To execute this streamlit app, you will need a lot of downloadings. To make it easier than setting up an environment, we used a notebook in order to have all the commands ready\n","\n","First, run the first cells in order to downloads every useful model and dataset, and write the streamlit file.\n","Then, to run streamlit within this notebook, run the cell under 'see address', and stop it right after. Copy the external IP address given as output. Run the cell below, click on the link, and give it that IP adress. You are now on the streamlit execution! "],"metadata":{"id":"K9t-zzXGrrp0"}},{"cell_type":"markdown","metadata":{"id":"gjIZYd1nLhe2"},"source":["# Setup repositery"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BY2AfYSIXcJp"},"outputs":[],"source":["import os\n","os.chdir('/content')\n","CODE_DIR = 'encoder4editing'\n","\n","!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n","!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n","!sudo unzip ninja-linux.zip -d /usr/local/bin/\n","!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n","os.chdir(f'./{CODE_DIR}')\n","\n","from argparse import Namespace\n","import time\n","import os\n","import sys\n","import numpy as np\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","\n","sys.path.append(\".\")\n","sys.path.append(\"..\")\n","\n","from utils.common import tensor2im\n","from models.psp import pSp  # we use the pSp framework to load the e4e encoder.\n","\n","%load_ext autoreload\n","%autoreload 2\n"]},{"cell_type":"markdown","metadata":{"id":"FNoNEohLLhe6"},"source":["# Setup files downloader\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Gy30CLbz3O38","pycharm":{"name":"#%%\n"},"executionInfo":{"status":"ok","timestamp":1685533757476,"user_tz":-120,"elapsed":25669,"user":{"displayName":"julie F","userId":"02970655246817770583"}}},"outputs":[],"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","download_with_pydrive = True #@param {type:\"boolean\"}\n","\n","class Downloader(object):\n","    def __init__(self, use_pydrive):\n","        self.use_pydrive = use_pydrive\n","        current_directory = os.getcwd()\n","        self.save_dir = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"pretrained_models\")\n","        os.makedirs(self.save_dir, exist_ok=True)\n","        if self.use_pydrive:\n","            self.authenticate()\n","\n","    def authenticate(self):\n","        auth.authenticate_user()\n","        gauth = GoogleAuth()\n","        gauth.credentials = GoogleCredentials.get_application_default()\n","        self.drive = GoogleDrive(gauth)\n","\n","    def download_file(self, file_id, file_name):\n","        file_dst = f'{self.save_dir}/{file_name}'\n","        if os.path.exists(file_dst):\n","            print(f'{file_name} already exists!')\n","            return\n","        if self.use_pydrive:\n","            downloaded = self.drive.CreateFile({'id':file_id})\n","            downloaded.FetchMetadata(fetch_all=True)\n","            downloaded.GetContentFile(file_dst)\n","        else:\n","            !gdown --id $file_id -O $file_dst\n","\n","downloader = Downloader(download_with_pydrive)"]},{"cell_type":"markdown","metadata":{"id":"4etDz82xkTJz"},"source":["## Download Pretrained Models \n","As part of this repository, we provide pretrained models for each of the above experiments. We'll download the model for the selected experiments and save it to the folder `pretrained_models`."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"KSnjlBZOkTJ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685535054113,"user_tz":-120,"elapsed":197,"user":{"displayName":"julie F","userId":"02970655246817770583"}},"outputId":"d8a2c7e9-b079-40c5-fa04-1848ed86766f"},"outputs":[{"output_type":"stream","name":"stdout","text":["e4e_ffhq_encode.pt already exists!\n"]}],"source":["# Download\n","MODEL_PATHS = {\n","    \"ffhq_encode\": {\"id\": \"1cUv_reLE6k3604or78EranS7XzuVMWeO\", \"name\": \"e4e_ffhq_encode.pt\"},\n","    \"cars_encode\": {\"id\": \"17faPqBce2m1AQeLCLHUVXaDfxMRU2QcV\", \"name\": \"e4e_cars_encode.pt\"},\n","    \"horse_encode\": {\"id\": \"1TkLLnuX86B_BMo2ocYD0kX9kWh53rUVX\", \"name\": \"e4e_horse_encode.pt\"},\n","    \"church_encode\": {\"id\": \"1-L0ZdnQLwtdy6-A_Ccgq5uNJGTqE7qBa\", \"name\": \"e4e_church_encode.pt\"}\n","}\n","\n","path = MODEL_PATHS[experiment_type]\n","downloader.download_file(file_id=path[\"id\"], file_name=path[\"name\"])"]},{"cell_type":"code","source":["if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n","    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2"],"metadata":{"id":"qY9vQUxos8ba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcJ1O1A5EpLk"},"source":["Install streamlit and localtunnel"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buwoQhawEVDR","outputId":"2f9eb631-7ff7-4ba1-b89c-ee2578062516","executionInfo":{"status":"ok","timestamp":1685533939776,"user_tz":-120,"elapsed":7787,"user":{"displayName":"julie F","userId":"02970655246817770583"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q streamlit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLt4lKYBEfCA","outputId":"e28f8d2a-1b6f-44f0-d0fa-f6aa70b8d672"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n","\u001b[0m\n","+ localtunnel@2.0.2\n","added 22 packages from 22 contributors and audited 22 packages in 2.279s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","found \u001b[92m0\u001b[0m vulnerabilities\n","\n","\u001b[K\u001b[?25h"]}],"source":["!npm install localtunnel"]},{"cell_type":"markdown","metadata":{"id":"h1tNA_b9EtFs"},"source":["## Create the Streamlit App\n","choose the dataset you want as a parameter"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3EQb72p_EYLV","outputId":"bbce723f-017f-4a48-c244-35d52df74f22","executionInfo":{"status":"ok","timestamp":1685534305703,"user_tz":-120,"elapsed":223,"user":{"displayName":"julie F","userId":"02970655246817770583"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/app.py\n"]}],"source":["%%writefile /content/app.py\n","import datetime\n","import os\n","import numpy as np\n","from argparse import Namespace\n","import streamlit as st\n","import time\n","import torch\n","import torchvision.transforms as transforms\n","from PIL import ImageOps, Image, ImageFont, ImageDraw\n","\n","import sys\n","\n","sys.path.append(\".\")\n","sys.path.append(\"..\")\n","\n","from utils.common import tensor2im\n","from models.psp import pSp  # we use the pSp framework to load the e4e encoder.\n","\n","# downloading pretrained models\n","ckpt = torch.load(\"pretrained_models/e4e_ffhq_encode.pt\", map_location='cpu')\n","opts = ckpt['opts']\n","# pprint.pprint(opts)  # Display full options used\n","# update the training options\n","opts['checkpoint_path'] = \"pretrained_models/e4e_ffhq_encode.pt\"\n","opts= Namespace(**opts)\n","net = pSp(opts)\n","net.eval()\n","net.cuda()\n","print('Model successfully loaded!')\n","\n","model2available_dataset = {\n","    \"stylegan1\": {\n","        \"FFHQ\": \"Gs_karras2019stylegan-ffhq-1024x1024.pt\",\n","        \"LSUN Bedroom\": \"Gs_karras2019stylegan-bedrooms-256x256.pt\",\n","        \"WikiArt Faces\": \"wikiart_faces.pt\",\n","    },\n","    \"stylegan2\": {\n","        \"FFHQ\": \"ffhq.pkl\",\n","        \"MetFaces\": \"metfaces.pkl\",\n","        \"LSUN Car\": \"stylegan2-car-config-f.pkl\",\n","        \"LSUN Horse\": \"stylegan2-horse-config-f.pkl\",\n","        \"LSUN Church\": \"stylegan2-church-config-f.pkl\",\n","    },\n","\n","}\n","\n","stylegan1_part2lelsd_model_paths = {\n","    #     \"FFHQ\": \"lelsd_stylegan1_ffhq\",\n","    #     \"WikiArt Faces\": \"lelsd_stylegan1_wikiart_faces\",\n","    #     \"LSUN Bedroom\": \"lelsd_stylegan1_lsun_bedroom\",\n","}\n","\n","stylegan2_part2lelsd_model_paths = {\n","    \"FFHQ\": \"lelsd_stylegan2_ffhq\",\n","    \"LSUN Car\": \"lelsd_stylegan2_lsun_car\",\n","    \"MetFaces\": \"lelsd_stylegan2_metfaces\",\n","    \"LSUN Horse\": \"lelsd_stylegan2_lsun_horse\",\n","    \"LSUN Church\": \"lelsd_stylegan2_lsun_church\",\n","}\n","\n","model2dataset2part2lelsd = {\n","    \"stylegan1\": stylegan1_part2lelsd_model_paths,\n","    \"stylegan2\": stylegan2_part2lelsd_model_paths,\n","}\n","\n","st.title(\"Inversion of picture to feed model\")\n","\n","def write_file_to_disk(uploaded_file):\n","    with open(\"temp.jpg\", \"wb\") as f:\n","        f.write(uploaded_file.getbuffer())\n","    return \"temp.jpg\"\n","\n","editing_image = write_file_to_disk(st.sidebar.file_uploader(\n","  \"Upload a picture in .jpg format\"\n","))\n","\n","model_name = st.sidebar.selectbox(\n","    \"Choose the GAN type you want.\",\n","    ['stylegan1', 'stylegan2'],\n",")\n","\n","dataset_name = st.sidebar.selectbox(\n","    \"Choose the dataset you want the pretrained model for\",\n","    list(model2available_dataset[model_name].keys()),\n",")\n","\n","truncation_psi = st.sidebar.slider(f'Truncation Psi', 0.01, 1.0, 0.7)  # min, max, default\n","\n","alpha_range_type = st.sidebar.selectbox(\n","    \"Choose the alpha range\",\n","    [\"normal\", \"extreme\"]\n",")\n","if alpha_range_type == \"normal\":\n","    min_value = -15\n","    max_value = 15\n","    value = (-5, 5)\n","else:\n","    min_value = -75\n","    max_value = 75\n","    value = (-30, 30)\n","\n","# Image process\n","### pre load\n","\n","experiment_type = 'ffhq_encode' #@param ['ffhq_encode', 'cars_encode', 'horse_encode', 'church_encode']\n","\n","EXPERIMENT_DATA_ARGS = {\n","    \"ffhq_encode\": {\n","        \"model_path\": \"pretrained_models/e4e_ffhq_encode.pt\",\n","        \"image_path\": editing_image\n","    },\n","    \"cars_encode\": {\n","        \"model_path\": \"pretrained_models/e4e_cars_encode.pt\",\n","        \"image_path\": \"notebooks/images/car_img.jpg\"\n","    },\n","    \"horse_encode\": {\n","        \"model_path\": \"pretrained_models/e4e_horse_encode.pt\",\n","        \"image_path\": \"notebooks/images/horse_img.jpg\"\n","    },\n","    \"church_encode\": {\n","        \"model_path\": \"pretrained_models/e4e_church_encode.pt\",\n","        \"image_path\": \"notebooks/images/church_img.jpg\"\n","    }\n","    \n","}\n","\n","EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n","if experiment_type == 'cars_encode':\n","    EXPERIMENT_ARGS['transform'] = transforms.Compose([\n","            transforms.Resize((192, 256)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","    resize_dims = (256, 192)\n","else:\n","    EXPERIMENT_ARGS['transform'] = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","    resize_dims = (256, 256)\n","\n","def run_alignment(image_path):\n","  import dlib\n","  from utils.alignment import align_face\n","  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n","  aligned_image = align_face(filepath=image_path, predictor=predictor) \n","  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n","  return aligned_image \n","\n","def run_on_batch(inputs, net):\n","    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n","    if experiment_type == 'cars_encode':\n","        images = images[:, :, 32:224, :]\n","    return images, latents\n","\n","### 1\n","image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\n","original_image = Image.open(image_path)\n","original_image = original_image.convert(\"RGB\")\n","\n","### 2 if ffhq, we need to first align the image to have the face centered\n","if experiment_type == \"ffhq_encode\":\n","  input_image = run_alignment(image_path)\n","else:\n","  input_image = original_image\n","\n","input_image.resize(resize_dims)\n","\n","### 3\n","img_transforms = EXPERIMENT_ARGS['transform']\n","transformed_image = img_transforms(input_image)\n","\n","### 4\n","with torch.no_grad():\n","    tic = time.time()\n","    images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n","    result_image, latent = images[0], latents[0]\n","    toc = time.time()\n","    print('Inference took {:.4f} seconds.'.format(toc - tic))\n","\n","\n","st.image(tensor2im(result_image), caption='2D Visualization of LELSD', use_column_width=False)"]},{"cell_type":"markdown","metadata":{"id":"yBzlsNrtHwF7"},"source":["See IP address"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sp-l4lYQHScK","outputId":"cd98e176-15a9-4a2a-ee78-23f9e5916651","executionInfo":{"status":"ok","timestamp":1685533990619,"user_tz":-120,"elapsed":27858,"user":{"displayName":"julie F","userId":"02970655246817770583"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n","\u001b[0m\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.172.181.100:8501\u001b[0m\n","\u001b[0m\n","\u001b[34m  Stopping...\u001b[0m\n","^C\n"]}],"source":["!streamlit run /content/app.py "]},{"cell_type":"markdown","metadata":{"id":"E_69uCq0Ev0O"},"source":["Run the App (open in 34.143.167.81 ) (or External URL above)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KroTUI3pEb9Y","outputId":"b125f973-fb1f-4bca-e5fd-749127679c9d","executionInfo":{"status":"ok","timestamp":1685534615589,"user_tz":-120,"elapsed":288417,"user":{"displayName":"julie F","userId":"02970655246817770583"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K\u001b[?25hnpx: installed 22 in 3.266s\n","your url is: https://fuzzy-friends-happen.loca.lt\n","^C\n"]}],"source":["!streamlit run /content/app.py &>/content/logs.txt &\n","!npx localtunnel --port 8501"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":0}